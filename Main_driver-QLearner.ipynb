{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Python3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym_examples\n",
    "import gym\n",
    "import numpy as np\n",
    "# from stable_baselines3 import PPO\n",
    "import sb3_contrib as sb3\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from gym.wrappers import FlattenObservation\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.7.0', '0.26.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb3.__version__, gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN ENV TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"gym_examples/CNN-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'layer_type': 0,\n",
       "  'layer_depth': 0,\n",
       "  'filter_depth': 0,\n",
       "  'filter_size': 0,\n",
       "  'fc_size': 0,\n",
       "  'is_start': 1,\n",
       "  'pool_size_and_stride': 0},\n",
       " {'current_network': [{'layer_type': 'conv',\n",
       "    'layer_depth': 0,\n",
       "    'filter_depth': 0,\n",
       "    'filter_size': 0,\n",
       "    'fc_size': 0,\n",
       "    'image_size': 28,\n",
       "    'pool_size': 0,\n",
       "    'pool_stride': 0,\n",
       "    'is_start': 1}],\n",
       "  'current_image_size': 28,\n",
       "  'current_layer_depth': 0,\n",
       "  'current_num_fc_layers': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_cnn = env.reset()\n",
    "start_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiDiscrete([3 9 5 4 5 2 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros(shape=(state_space, action_space))\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    return action\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    random_num = random.uniform(0, 1)\n",
    "    if random_num > epsilon:\n",
    "        action = greedy_policy(Qtable, state)\n",
    "    else:\n",
    "        # TODO: need to change to a valid action\n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable, learning_rate, gamma, replay_memory_size):\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        replay_memory = []\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        print(\"Start:\", state)\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "            # TODO: !!!!!!!!!!!!!!!!!!!! SAMPLE VALID ACTION AND THEN DO epsilon greedy\n",
    "\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "            print(\"Action:\",action)\n",
    "            # Take action At and observe Rt+1 and St+1\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, _, info = env.step(action)\n",
    "            print(\"Next state:\", new_state, \" reward:\", reward)\n",
    "            replay_memory.append((new_state, action, reward))\n",
    "\n",
    "            for memory in range(replay_memory_size):\n",
    "                # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "                new_state, action, reward = random.choice(np.array(replay_memory))\n",
    "                new_state = new_state.get(\"r2d2\")\n",
    "                new_state = new_state[0] * 5 + new_state[1]\n",
    "                print(new_state)\n",
    "                # handle termination here...\n",
    "                Qtable[nestate][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "            # If done, finish the episode\n",
    "            if done:\n",
    "                break\n",
    "            # Our next state is the new state\n",
    "            state = new_state\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict('fc_size': Discrete(5), 'filter_depth': Discrete(5), 'filter_size': Discrete(4), 'is_start': Discrete(2), 'layer_depth': Discrete(9), 'layer_type': Discrete(3), 'pool_size_and_stride': Discrete(4)),\n",
       " MultiDiscrete([3 9 5 4 5 2 4]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space = env.observation_space\n",
    "\n",
    "action_space = env.action_space\n",
    "\n",
    "state_space, action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf73a689ba998dc6f3460086dc66de4c3a85bb66803eb508340fb019598e645e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
