# -*- coding: utf-8 -*-
"""pytorch-parser-function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BUg1tfReM7GuHCPipgwj5NiQgkZ0MegO
"""
import os
import datetime
import json
import torch
from torch.utils.data import DataLoader, Dataset
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.utils.data import DataLoader

from torch import optim
from torch.autograd import Variable

import numpy as np
from torchvision import datasets
from torchvision.transforms import ToTensor, Compose, Normalize

torch.manual_seed(1)

device=torch.device(0) if torch.cuda.is_available() else 'cpu'


# NOTE: Make work for list params for image size, filter size etc (i.e. non square inputs) -> assuming all square currently

# TODO: Account for PADDING -> assuming 0 for now


class GenerateCNN(nn.Module):
    def __init__(self, network_tuples, n_init_channels, n_classes=3, input_image_size=28):
      '''
      network_tuples: list of tuples defining network. Params:
        layer_type,        # String -- conv, pool, fc, softmax
        layer_depth,       # Current depth of network
        filter_depth,      # Used for conv, 0 when not conv
        filter_size,       # Used for conv and pool, 0 otherwise
        stride,            # Used for conv and pool, 0 otherwise
        image_size,        # Used for any layer that maintains square input (conv and pool), 0 otherwise
        fc_size,           # Used for fc and softmax -- number of neurons in layer
        terminate,
        state_list 
      n_init_channels: no of channels in dataset images
      '''
      super(GenerateCNN, self).__init__()
      self.layers=nn.ModuleList()

      in_filter=None
      running_image_size=input_image_size
      firstFC=True
      running_fc_size=None

      for i,state in enumerate(network_tuples):
        
        layer_type, layer_depth, filter_depth, filter_size, stride, image_size, fc_size, terminate, state_list = state
        # print(layer_type, layer_depth, filter_depth, filter_size, stride, image_size, fc_size, terminate, state_list)
        # print('running img size',running_image_size, 'infilters',in_filter)
        if layer_type=='conv':
          assert filter_depth != 0, "filter_depth to conv layer must be non-zero"

          if i==0:
            self.layers.append(nn.Conv2d(n_init_channels,filter_depth, filter_size, stride))
            # running_image_size=image_size
          else:
            self.layers.append(nn.Conv2d(in_filter,filter_depth, filter_size, stride))
          in_filter=filter_depth
          running_image_size=self._calc_new_image_size(running_image_size, filter_size, stride)


        elif layer_type=='pool':
          if i==0:
            # self.layers.append(nn.MaxPool2d(filter_size, stride))
            running_image_size=image_size
            in_filter=n_init_channels

          self.layers.append(nn.MaxPool2d(filter_size, stride))
          running_image_size=self._calc_new_image_size(running_image_size, filter_size, stride)


        elif layer_type=='fc':

          # assert image_size == 0, "Image size to fc layer must be 0, is {}".format(image_size)
          if firstFC:
            firstFC=False
            # print('image size',running_image_size)
            # print('infilter',in_filter)
            image_size_unroll=running_image_size**2 # NOTE: assumption of only square images
            image_size_unroll*=in_filter
            # print('Unrolled img size:',image_size_unroll)
            # self.layers.append(nn.Flatten())
            self.layers.append(nn.Linear(image_size_unroll,fc_size))
            
          else:
            self.layers.append(nn.Linear(running_fc_size,fc_size))
          running_fc_size=fc_size
      
      # get the last layer and check
      # if lastlayer == FC AND has 3 out: Do nothing
      # if lastlayer == FC But not 3 out: Add FC with 3 Out
      # if lastlayer != FC -> Flatten -> Add FC with 3 Out
      lastlayer=self.layers[-1]
      if (isinstance(lastlayer, nn.Linear) and lastlayer.out_features!=n_classes):
          # print(lastlayer.out_features)
          # self.layers.append(nn.Flatten())
          self.layers.append(nn.Linear(lastlayer.out_features, n_classes))
          
      elif isinstance(lastlayer, nn.Conv2d) or isinstance(lastlayer, nn.MaxPool2d):
          self.layers.append(nn.Flatten())
          image_size_unroll=running_image_size**2 # NOTE: assumption of only square images
          image_size_unroll*=in_filter
          # print('Unrolled img size:',image_size_unroll)

          self.layers.append(nn.Linear(image_size_unroll,n_classes))         
        
        


      # https://github.com/bowenbaker/metaqnn/blob/a25847f635e9545455f83405453e740646038f7a/libs/grammar/state_enumerator.py#L207
    
    
    @staticmethod
    def _generate_layer(state, running_image_size, in_filter, firstFC, running_fc_size):
      # in_filter=None
      # running_image_size=self.input_image_size
      # firstFC=True
      # running_fc_size=None

      layer_type, layer_depth, filter_depth, filter_size, stride, image_size, fc_size, terminate, state_list = state
      if layer_type=='conv':
        assert filter_depth != 0, "filter_depth to conv layer must be non-zero"
        # if i==0:
        #   return nn.Conv2d(self.n_init_channels,filter_depth, filter_size, stride)
        #   # self.layers.append()
        #   # running_image_size=image_size
        # else:
        return nn.Conv2d(in_filter,filter_depth, filter_size, stride)

      elif layer_type=='pool':
        return nn.MaxPool2d(filter_size, stride)
        

      elif layer_type=='fc':

        return nn.Linear(running_fc_size,fc_size)
    
    def _calc_new_image_size(self, image_size, filter_size, stride):
      '''Returns new image size given previous image size and filter parameters'''
      new_size = int(math.ceil(float(image_size - filter_size + 1) / float(stride)))
      return new_size

    def forward(self, x):
      
      for layer in self.layers:
        if isinstance(layer, nn.Conv2d):
          layer_type='conv'
        elif isinstance(layer, nn.MaxPool2d):
          layer_type='mp'
        elif isinstance(layer, nn.Linear):
          layer_type='fc'

          x=x.view(x.shape[0],-1) #flatten before a linear layer
        # print(layer_type, x.shape)
        x=layer(x)
        # print('After',x.shape)
      
      return x
    
# https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/24
def n_params(model):
  return sum(p.numel() for p in model.parameters() if p.requires_grad)
       


def generate_and_train(model_architecture, train_data, test_data, data_path=None, run_name=None, dataset_name="mnist", n_classes=3, lr=0.01, num_epochs=5, verbose = False):
  input_channels, input_image_size=1,28
  if "cifar" in dataset_name:
     input_channels, input_image_size=3,32
  model = GenerateCNN(model_architecture, input_channels, n_classes, input_image_size)
  model_size=n_params(model)
  if verbose:
    print(model)
    print('Number of trainable parameters: %d' % (model_size))
    
  model.to(device)

  loaders = {
    'train' : torch.utils.data.DataLoader(train_data, 
                                          batch_size=100, 
                                          shuffle=True, 
                                          num_workers=1),

    'test'  : torch.utils.data.DataLoader(test_data, 
                                          batch_size=100, 
                                          shuffle=True, 
                                          num_workers=0),
  }


  loss_func = nn.CrossEntropyLoss()   
  optimizer = optim.Adam(model.parameters(), lr = lr)   


  model.train()
  if verbose:
    print("TRAINING")
  # Train the model
  total_step = len(loaders['train'])
      
  for epoch in range(num_epochs):
      for i, (images, labels) in enumerate(loaders['train']):
          
          # gives batch data, normalize x when iterate train_loader
          b_x = Variable(images).to(device)   # batch x
          b_y = Variable(labels).to(device)   # batch y
          # print('y batch shape,',b_y.shape)

          output = model(b_x)

          loss = loss_func(output, b_y)
          
          # clear gradients for this training step   
          optimizer.zero_grad()           
          
          # backpropagation, compute gradients 
          loss.backward()    
          # apply gradients             
          optimizer.step()                
          
          if verbose:
            if (i+1) % 100 == 0:
                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))

  # Test the model
  model.eval()
  if verbose:
    print('TESTING')
  with torch.no_grad():
      correct = 0
      total = 0
      accuracy=0
      for images, labels in loaders['test']:
          images=images.to(device)
          labels=labels.to(device)
          test_output= model(images)
          pred_y = torch.max(test_output, 1)[1].data.squeeze()
          correct += (pred_y == labels).sum().item()
          total+=float(labels.size(0))
  accuracy=correct/total
  if verbose:        
    print('Test Accuracy of the model on test images: %.4f' % (accuracy))

  if data_path:
    timestamp=int(datetime.datetime.now().timestamp())

    filename=os.path.join(data_path,"{}_{}_{}_{}.json".format(dataset_name, n_classes, run_name,str(timestamp)))
    write_dict={
      "network_tuples":model_architecture,
      "accuracy": accuracy,
      "model_size":model_size,
      "dataset":dataset_name
    }
    with open(filename, 'w+') as f:
      json.dump(write_dict, f)

  return accuracy, model_size

# https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118

if __name__=="__main__":

  train_data = datasets.MNIST(
      root = 'data',
      train = True,                         
      transform = ToTensor(), 
      download = True,            
  )

  test_data = datasets.MNIST(
      root = 'data', 
      train = False, 
      transform = ToTensor()
  )

  transform = Compose(
    [ToTensor(),
     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
  
  cifar_trainset = datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)

  cifar_testset = datasets.CIFAR10(root='./data', train=False,
                                        download=True, transform=transform)
  

  #  MNIST_10
  # generate_and_train([('conv',1,5,3,1,28,0,0,[]),('pool',1,0,2,2,28,0,0,[]),('conv',1,10,3,1,26,0,0,[]), \
  #                     ('pool',1,0,2,2,28,0,0,[]),('fc',0,0,0,0,0,100,0,[]),('fc',0,0,0,0,0,10,0,[])], \
  #                       train_data, test_data, n_classes=10, num_epochs=1, verbose=True)
  
  # CIFAR
  # generate_and_train(  [('conv', 1, 20, 3, 1, 26, 0, 0, []), ('pool', 2, 20, 2, 2, 13, 512, 0, []), ('conv', 3, 32, 3, 1, 11, 512, 0, []), ('fc', 4, 10, 3, 2, 11, 64, 0, [])],
  #                       cifar_trainset, cifar_testset, dataset_name='cifar', n_classes=10, num_epochs=1, verbose=True, data_path=".", run_name="1")
  
  # generate_and_train(  [('conv', 1, 20, 3, 1, 32, 0,0,[]),('conv', 2, 10, 5, 1, 30, 0,0,[]),('pool', 3, 0, 2, 2, 111, 0,0,[]),('fc', 4, 20, 3, 1, 111, 512,1,[])],
  #                       cifar_trainset, cifar_testset, dataset_name='cifar', n_classes=10, num_epochs=1, verbose=True, data_path=".", run_name="1")
  

