# -*- coding: utf-8 -*-
"""pytorch-parser-function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BUg1tfReM7GuHCPipgwj5NiQgkZ0MegO
"""

import torch
from torch.utils.data import DataLoader, Dataset
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.utils.data import DataLoader

from torch import optim
from torch.autograd import Variable

import numpy as np
from torchvision import datasets
from torchvision.transforms import ToTensor

torch.manual_seed(1)

device=torch.device(0) if torch.cuda.is_available() else 'cpu'

# TODO: Account for PADDING
# NOTE: Make work for list params for image size, filter size etc (i.e. non square inputs) -> assuming all square currently

class GenerateCNN(nn.Module):
    def __init__(self, network_tuples, n_init_channels, n_classes=3):
      '''
      network_tuples: list of tuples defining network. Params:
        layer_type,        # String -- conv, pool, fc, softmax
        layer_depth,       # Current depth of network
        filter_depth,      # Used for conv, 0 when not conv
        filter_size,       # Used for conv and pool, 0 otherwise
        stride,            # Used for conv and pool, 0 otherwise
        image_size,        # Used for any layer that maintains square input (conv and pool), 0 otherwise
        fc_size,           # Used for fc and softmax -- number of neurons in layer
        terminate,
        state_list 
      n_init_channels: no of channels in dataset images
      '''
      super(GenerateCNN, self).__init__()
      self.layers=nn.ModuleList()

      in_filter=None
      running_image_size=None
      firstFC=True
      running_fc_size=None

      for i,state in enumerate(network_tuples):
        
        layer_type, layer_depth, filter_depth, filter_size, stride, image_size, fc_size, terminate, state_list = state
        
        # print(running_image_size, in_filter)
        if layer_type=='conv':
          assert filter_depth != 0, "filter_depth to conv layer must be non-zero"

          if i==0:
            self.layers.append(nn.Conv2d(n_init_channels,filter_depth, filter_size, stride))
            running_image_size=image_size
          else:
            self.layers.append(nn.Conv2d(in_filter,filter_depth, filter_size, stride))
          in_filter=filter_depth
          running_image_size=self._calc_new_image_size(running_image_size, filter_size, stride)


        elif layer_type=='pool':
          self.layers.append(nn.MaxPool2d(filter_size, stride))
          running_image_size=self._calc_new_image_size(running_image_size, filter_size, stride)


        elif layer_type=='fc':

          # assert image_size == 0, "Image size to fc layer must be 0, is {}".format(image_size)
          if firstFC:
            firstFC=False
            image_size_unroll=running_image_size**2 # NOTE: assumption of only square images
            image_size_unroll*=in_filter
            # print('Unrolled img size:',image_size_unroll)
            # self.layers.append(nn.Flatten())
            self.layers.append(nn.Linear(image_size_unroll,fc_size))
            
          else:
            self.layers.append(nn.Linear(running_fc_size,fc_size))
          running_fc_size=fc_size
      
      # get the last layer and check
      # if lastlayer == FC AND has 3 out: Do nothing
      # if lastlayer == FC But not 3 out: Add FC with 3 Out
      lastlayer=self.layers[-1]
      if (isinstance(lastlayer, nn.Linear) and lastlayer.out_features!=n_classes):
          print(lastlayer.out_features)
          self.layers.append(nn.Linear(lastlayer.out_features, n_classes))
          
      # if lastlayer != FC -> Flatten -> Add FC with 3 Out
      elif isinstance(lastlayer, nn.Conv2d) or isinstance(lastlayer, nn.MaxPool2d):
          self.layers.append(nn.Flatten())
          image_size_unroll=running_image_size**2 # NOTE: assumption of only square images
          image_size_unroll*=in_filter

          self.layers.append(nn.Linear(image_size_unroll,n_classes))         
        
    
    # https://github.com/bowenbaker/metaqnn/blob/a25847f635e9545455f83405453e740646038f7a/libs/grammar/state_enumerator.py#L207
    def _calc_new_image_size(self, image_size, filter_size, stride):
      '''Returns new image size given previous image size and filter parameters'''
      new_size = int(math.ceil(float(image_size - filter_size + 1) / float(stride)))
      return new_size

    def forward(self, x):
      for layer in self.layers:
        if isinstance(layer, nn.Conv2d):
          layer_type='conv'
        elif isinstance(layer, nn.MaxPool2d):
          layer_type='mp'
        elif isinstance(layer, nn.Linear):
          x=x.view(x.shape[0],-1) #flatten before a linear layer
        x=layer(x)
        # print(layer_type, x.shape)
      
      return x

def generate_and_train(model_architecture, train_data, test_data, input_channels=1, lr=0.01, num_epochs=5, verbose = False):

  model = GenerateCNN(model_architecture, input_channels)
  if verbose:
    print(model)
  model.to(device)

  loaders = {
    'train' : torch.utils.data.DataLoader(train_data, 
                                          batch_size=100, 
                                          shuffle=True, 
                                          num_workers=1),

    'test'  : torch.utils.data.DataLoader(test_data, 
                                          batch_size=100, 
                                          shuffle=True, 
                                          num_workers=0),
  }


  loss_func = nn.CrossEntropyLoss()   
  optimizer = optim.Adam(model.parameters(), lr = lr)   


  model.train()
  if verbose:
    print("TRAINING")
  # Train the model
  total_step = len(loaders['train'])
      
  for epoch in range(num_epochs):
      for i, (images, labels) in enumerate(loaders['train']):
          
          # gives batch data, normalize x when iterate train_loader
          b_x = Variable(images).to(device)   # batch x
          b_y = Variable(labels).to(device)   # batch y
          # print('y batch shape,',b_y.shape)

          output = model(b_x)

          loss = loss_func(output, b_y)
          
          # clear gradients for this training step   
          optimizer.zero_grad()           
          
          # backpropagation, compute gradients 
          loss.backward()    
          # apply gradients             
          optimizer.step()                
          
          if verbose:
            if (i+1) % 100 == 0:
                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))

  # Test the model
  model.eval()
  if verbose:
    print('TESTING')
  with torch.no_grad():
      correct = 0
      total = 0
      accuracy=0
      for images, labels in loaders['test']:
          images=images.to(device)
          labels=labels.to(device)
          test_output= model(images)
          pred_y = torch.max(test_output, 1)[1].data.squeeze()
          accuracy += (pred_y == labels).sum().item()
          total+=float(labels.size(0))

  if verbose:        
    print('Test Accuracy of the model on the 10000 test images: %.4f' % (accuracy/total))
  return accuracy/total

# https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118

if __name__=="__main__":

  train_data = datasets.MNIST(
      root = 'data',
      train = True,                         
      transform = ToTensor(), 
      download = True,            
  )

  test_data = datasets.MNIST(
      root = 'data', 
      train = False, 
      transform = ToTensor()
  )

  generate_and_train([('conv',1,5,3,1,28,0,0,[]),('pool',1,0,2,2,28,0,0,[]),('conv',1,10,3,1,26,0,0,[]),('pool',1,0,2,2,28,0,0,[]),('fc',0,0,0,0,0,100,0,[]),('fc',0,0,0,0,0,10,0,[])], train_data, test_data)